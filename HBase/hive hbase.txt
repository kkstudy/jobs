CREATE TABLE `kylin_country_hbase`(
  `country` string, 
  `latitude` double, 
  `longitude` double, 
  `name` string)
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:latitude,cf1:longitude,cf1:name")
TBLPROPERTIES ("hbase.table.name" = "kylin_country", "hbase.mapred.output.outputtable" = "kylin_country_mapout");


INSERT INTO TABLE kylin_country_hbase(country, latitude, longitude, name) SELECT * FROM kylin_country;
INSERT INTO TABLE kylin_country_hbase  SELECT * FROM kylin_country;



Spark SQL  
将hive lib下HBase相关的jar文件链接到 spark jars路径下： ll /data/bigdata/hive/lib/*hbase* | awk '{ print "ln -s "$9" ." }'
ln -s /data/bigdata/hive/lib/hbase-annotations-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-client-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-common-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-common-1.1.1-tests.jar .
ln -s /data/bigdata/hive/lib/hbase-hadoop2-compat-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-hadoop2-compat-1.1.1-tests.jar .
ln -s /data/bigdata/hive/lib/hbase-hadoop-compat-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-prefix-tree-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-procedure-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-protocol-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hbase-server-1.1.1.jar .
ln -s /data/bigdata/hive/lib/hive-hbase-handler-2.1.1.jar .
ln -s /data/bigdata/hive/lib/tephra-hbase-compat-1.0-0.6.0.jar .

重新生成并上传 spark.yarn.archive 文件
jar cv0f spark-2.1.0.jar -C jars/ .

复制 hbase-site.xml 到 Spark conf 路径下



CREATE EXTERNAL TABLE articlecontent_txt(
`id` int, 
`Content` string)
row format delimited fields terminated by '\t'
LOCATION '/orgs/csdn/data/enterprise/blog/articlecontent';

create 'articlecontent','cf1',{ NUMREGIONS => 10 ,SPLITALGO => 'HexStringSplit' }
count 'articlecontent'
truncate 'articlecontent'

CREATE EXTERNAL TABLE `articlecontent`(
  `id` string, 
  `Content` string)
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:Content")
TBLPROPERTIES ("hbase.table.name" = "articlecontent");


add jar /data/1/usr/local/hive/lib/hbase-common-1.1.1.jar;
add jar /home/hadoop/hbase-rowkey-1.0-SNAPSHOT.jar;
CREATE TEMPORARY FUNCTION HBaseSequenceId AS 'net.csdn.udf.HBaseSequenceIdRowKeyUDF';

insert OVERWRITE table articlecontent select HBaseSequenceId(id) as id,Content from articlecontent_txt;
