sqoop:000> show connector
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
+------------------------+----------+------------------------------------------------------------+----------------------+
|          Name          | Version  |                           Class                            | Supported Directions |
+------------------------+----------+------------------------------------------------------------+----------------------+
| generic-jdbc-connector | 1.99.7.1 | org.apache.sqoop.connector.jdbc.GenericJdbcConnector       | FROM/TO              |
| kite-connector         | 1.99.7.1 | org.apache.sqoop.connector.kite.KiteConnector              | FROM/TO              |
| oracle-jdbc-connector  | 1.99.7.1 | org.apache.sqoop.connector.jdbc.oracle.OracleJdbcConnector | FROM/TO              |
| ftp-connector          | 1.99.7.1 | org.apache.sqoop.connector.ftp.FtpConnector                | TO                   |
| hdfs-connector         | 1.99.7.1 | org.apache.sqoop.connector.hdfs.HdfsConnector              | FROM/TO              |
| kafka-connector        | 1.99.7.1 | org.apache.sqoop.connector.kafka.KafkaConnector            | TO                   |
| sftp-connector         | 1.99.7.1 | org.apache.sqoop.connector.sftp.SftpConnector              | TO                   |
+------------------------+----------+------------------------------------------------------------+----------------------+


sqoop:000> create link -connector generic-jdbc-connector
Creating link for connector with name generic-jdbc-connector
Please fill following values to create new link object
Name: codedata_jdbc_link

Database connection

Driver class: com.mysql.jdbc.Driver
Connection String: jdbc:mysql://192.168.6.145:3306/codedata
Username: root
Password: ****
Fetch Size: 100                                    ----java.sql.Statement#setFetchSize Gives the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed for ResultSet objects generated by this Statement. If the value specified is zero, then the hint is ignored. The default value is zero.
Connection Properties: 
There are currently 0 values in the map:
entry#                                             -----Map 数据结构entry的意思

SQL Dialect                                        ------数据库方言

Identifier enclose: `                               -------Character(s) that should be used to enclose table name, schema or column names.
New link was successfully created with validation status OK and name codedata_jdbc_link


sqoop:001> create link -c hdfs-connector
Creating link for connector with name hdfs-connector
Please fill following values to create new link object
Name: codedata_hdfs_link

HDFS cluster

URI: hdfs://csdncluster
Conf directory: /data/bigdata/hadoop/etc/hadoop
Additional configs:: 
There are currently 0 values in the map:
entry# 
New link was successfully created with validation status OK and name codedata_hdfs_link


sqoop:001> show link
+--------------------+------------------------+---------+
|        Name        |     Connector Name     | Enabled |
+--------------------+------------------------+---------+
| codedata_jdbc_link | generic-jdbc-connector | true    |
| codedata_hdfs_link | hdfs-connector         | true    |
+--------------------+------------------------+---------+
sqoop:001>  show link --all
2 link(s) to show: 
link with name codedata_jdbc_link (Enabled: true, Created by hadoop at 4/13/17 5:27 PM, Updated by hadoop at 4/13/17 5:53 PM)
Using Connector generic-jdbc-connector with name {1}
  Database connection
    Driver class: com.mysql.jdbc.Driver
    Connection String: jdbc:mysql://192.168.6.145:3306/codedata
    Username: root
    Password: 
    Fetch Size: 100
    Connection Properties: 
  SQL Dialect
    Identifier enclose: `
link with name codedata_hdfs_link (Enabled: true, Created by hadoop at 4/13/17 5:50 PM, Updated by hadoop at 4/13/17 5:50 PM)
Using Connector hdfs-connector with name {1}
  HDFS cluster
    URI: hdfs://csdncluster
    Conf directory: /data/bigdata/hadoop/etc/hadoop
    Additional configs:: 

sqoop:001> create job -f "codedata_jdbc_link" -t "codedata_hdfs_link"
Creating job for links with from name codedata_jdbc_link and to name codedata_hdfs_link
Please fill following values to create new job object
Name: load_codedata_to_hdfs

Database source

Schema name: codedata
Table name: access_log
SQL statement: 
Column names: 
There are currently 0 values in the list:
element# 
Partition column: id
Partition column nullable: true
Boundary query: 

Incremental read

Check column: 
Last value: 

Target configuration

Override null value: 
Null value: 
File format: 
  0 : TEXT_FILE
  1 : SEQUENCE_FILE
  2 : PARQUET_FILE
Choose: 0
Compression codec: 
  0 : NONE
  1 : DEFAULT
  2 : DEFLATE
  3 : GZIP
  4 : BZIP2
  5 : LZO
  6 : LZ4
  7 : SNAPPY
  8 : CUSTOM
Choose: 0
Custom codec: 
Output directory: /data/log/codedata
Append mode: 
Delete output directory: 

Throttling resources

Extractors: 
Loaders: 

Classpath configuration

Extra mapper jars: 
There are currently 0 values in the list:
element# 
New job was successfully created with validation status OK  and name load_codedata_to_hdfs
sqoop:001> 





mvn package -DskipTests -Pbinary -Dhadoop.profile=200